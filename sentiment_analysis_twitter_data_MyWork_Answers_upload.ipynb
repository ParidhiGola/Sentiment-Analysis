{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FU-HwvIdH0M-"
   },
   "source": [
    "## Sentiment analysis <br> \n",
    "\n",
    "The objective of this problem is to perform Sentiment analysis from the tweets data collected from the users targeted at various mobile devices.\n",
    "Based on the tweet posted by a user (text), we will classify if the sentiment of the user targeted at a particular mobile device is positive or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nAQDiZHRH0M_"
   },
   "source": [
    "### 1. Read the dataset (tweets.csv) and drop the NA's while reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change directory\n",
    "import os\n",
    "os.chdir('G:\\Paridhi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3eXGIe-SH0NA"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('tweets.csv',encoding= 'unicode_escape').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CWeWe1eJH0NF"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iPhone   \n",
       "1              iPad or iPhone App   \n",
       "2                            iPad   \n",
       "3              iPad or iPhone App   \n",
       "4                          Google   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                   Negative emotion  \n",
       "1                                   Positive emotion  \n",
       "2                                   Positive emotion  \n",
       "3                                   Negative emotion  \n",
       "4                                   Positive emotion  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jPJvTjefH0NI"
   },
   "source": [
    "### 2. Preprocess the text and add the preprocessed text in a column with name `text` in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5iec5s9gH0NI"
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    try:\n",
    "        return ''.join(i if ord(i)<128 else ' ' for i in text)\n",
    "    except Exception as e:\n",
    "        return \" \"\n",
    "\n",
    "data['text'] = [preprocess(text) for text in data.tweet_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EQSmqA-vH0NT"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iPhone   \n",
       "1              iPad or iPhone App   \n",
       "2                            iPad   \n",
       "3              iPad or iPhone App   \n",
       "4                          Google   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \\\n",
       "0                                   Negative emotion   \n",
       "1                                   Positive emotion   \n",
       "2                                   Positive emotion   \n",
       "3                                   Negative emotion   \n",
       "4                                   Positive emotion   \n",
       "\n",
       "                                                text  \n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...  \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...  \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...  \n",
       "3  @sxsw I hope this year's festival isn't as cra...  \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7kX-WoJDH0NV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OGWB3P2WH0NY"
   },
   "source": [
    "### 3. Consider only rows having a Positive or Negative emotion and remove other rows from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bdgA_8N2H0NY"
   },
   "outputs": [],
   "source": [
    "data = data[(data['is_there_an_emotion_directed_at_a_brand_or_product'] == 'Negative emotion') | (data['is_there_an_emotion_directed_at_a_brand_or_product'] == 'Positive emotion')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3191 entries, 0 to 9088\n",
      "Data columns (total 4 columns):\n",
      "tweet_text                                            3191 non-null object\n",
      "emotion_in_tweet_is_directed_at                       3191 non-null object\n",
      "is_there_an_emotion_directed_at_a_brand_or_product    3191 non-null object\n",
      "text                                                  3191 non-null object\n",
      "dtypes: object(4)\n",
      "memory usage: 124.6+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SotCRvkDH0Nf"
   },
   "source": [
    "### 4. Represent text as numerical data using `CountVectorizer` and get the document term frequency matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YcbkY4sgH0Ng"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "cv.fit(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KyXtZGr-H0Nl"
   },
   "outputs": [],
   "source": [
    "doc_matrix = cv.transform(data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z4LUM-XPH0Nn"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3191x5610 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 53151 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aIdZYxJtH0Nq"
   },
   "outputs": [],
   "source": [
    "doc_matrix1 = doc_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3191, 5610)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_matrix1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5pxd5fSHH0Nt"
   },
   "source": [
    "### 5. Find number of different words in vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9912"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Method1 total number of words\n",
    "from collections import Counter\n",
    "r = Counter(\" \".join(data['text']).split(\" \")).items()\n",
    "len(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8619"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Method2 total number of unique words - different because of adding lower()\n",
    "uniqueWords = list(set(\" \".join(data['text']).lower().split(\" \")))\n",
    "count = len(uniqueWords)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'intro,',\n",
       " '#crazyco',\n",
       " 'fr',\n",
       " '&quot;not',\n",
       " 'product...',\n",
       " 'experts:',\n",
       " 'wilderness',\n",
       " \"company's\",\n",
       " 'more.',\n",
       " 'class.',\n",
       " '*slips',\n",
       " 'ordered',\n",
       " '#wssxsw',\n",
       " 'cab',\n",
       " 'please?',\n",
       " 'drowning',\n",
       " 'device.&quot;',\n",
       " 'visiting',\n",
       " \"frickin'\",\n",
       " 'destroyed',\n",
       " 'having',\n",
       " 'realize',\n",
       " 'plied',\n",
       " 'arm.',\n",
       " 'fanboys.',\n",
       " 'dropped',\n",
       " 'guides',\n",
       " 'convience',\n",
       " 'come?',\n",
       " 'amazon.',\n",
       " 'goddamn',\n",
       " 'yelping!!',\n",
       " '#smileyparty',\n",
       " '#iphone,',\n",
       " 'installs',\n",
       " 'etc.',\n",
       " 'down',\n",
       " 'center,',\n",
       " 'may',\n",
       " 'asked,',\n",
       " '98.5%',\n",
       " 'good.',\n",
       " 'touched',\n",
       " 'one!',\n",
       " 'version',\n",
       " \"registrant's\",\n",
       " 'conquered.',\n",
       " 'flip',\n",
       " 'enchanting,',\n",
       " 'native',\n",
       " \"won't\",\n",
       " 'finder',\n",
       " 'tuned',\n",
       " 'see-really',\n",
       " 'day.from',\n",
       " 'geo-location',\n",
       " 'popular',\n",
       " 'perhaps',\n",
       " 'expect.',\n",
       " 'mean,',\n",
       " 'replacement',\n",
       " 'true:',\n",
       " 'mindjet',\n",
       " 'interface',\n",
       " '#futuremf',\n",
       " 'pen.',\n",
       " 'seats.',\n",
       " 'discuss',\n",
       " '*strums',\n",
       " 'shuffling',\n",
       " 'free:',\n",
       " 'table',\n",
       " 'classiest,',\n",
       " 'droid,',\n",
       " 'you,',\n",
       " 'source',\n",
       " 'yrs',\n",
       " '#h4ckers',\n",
       " 't-mobile',\n",
       " 'yet:',\n",
       " 'asddieu',\n",
       " \"it'll\",\n",
       " 'tempt',\n",
       " 'forgo',\n",
       " 'week,',\n",
       " 'partytweets',\n",
       " 'republic',\n",
       " 'flights',\n",
       " 'only.',\n",
       " 'restaurants',\n",
       " 'fyi...]',\n",
       " 'text',\n",
       " 'awesome',\n",
       " 'feature?',\n",
       " 'ipad2.',\n",
       " 'sesh',\n",
       " 'bucket.',\n",
       " 'lanyrd',\n",
       " 'mister',\n",
       " 'ipad/ipod.',\n",
       " 'interviewed',\n",
       " 'corrupted',\n",
       " '#myegc',\n",
       " '6th.',\n",
       " '(cont',\n",
       " 'sad',\n",
       " 'experiment',\n",
       " 'xoom',\n",
       " 'chevy',\n",
       " 'room#nokiaconnects',\n",
       " 'circle!',\n",
       " '2s?',\n",
       " 'most',\n",
       " 'fondling',\n",
       " 'garageband',\n",
       " 'excludes',\n",
       " 'you',\n",
       " '100%.',\n",
       " '#youtube',\n",
       " 'v1',\n",
       " '2nd',\n",
       " 'note,',\n",
       " 'great!',\n",
       " 'pdx',\n",
       " '--&gt;',\n",
       " 'caramel',\n",
       " 'excellent',\n",
       " 'don',\n",
       " 'killer',\n",
       " '#eventseekr',\n",
       " 'bad',\n",
       " 'top',\n",
       " 'pocket!',\n",
       " 'story/meaning',\n",
       " '@mentionn',\n",
       " 'dell',\n",
       " 'no,',\n",
       " 'evidence',\n",
       " 'dropping.',\n",
       " 'process,',\n",
       " '#geeksrule',\n",
       " 'artificial',\n",
       " 'go2',\n",
       " '#musiek',\n",
       " 'changes',\n",
       " 'julian',\n",
       " 'admits',\n",
       " 'acquired.',\n",
       " 'sunny',\n",
       " 'convore',\n",
       " \"y'all\",\n",
       " 'guys.',\n",
       " 'true,expect',\n",
       " 'rad',\n",
       " 'q&amp;a',\n",
       " 'for:',\n",
       " 'shallow',\n",
       " 'titles&quot;',\n",
       " 'recommendations',\n",
       " 'donate',\n",
       " 'sweet...apple',\n",
       " 'cart',\n",
       " '10,000',\n",
       " 'mothers',\n",
       " 'chip',\n",
       " '&quot;mistakes',\n",
       " 'buildings',\n",
       " 'jaw',\n",
       " 'overload',\n",
       " 'major',\n",
       " 'first!)',\n",
       " 'heads',\n",
       " 'prob',\n",
       " 'new',\n",
       " 'hide',\n",
       " 'shortcuts',\n",
       " 'djroe',\n",
       " '2s!',\n",
       " 'cost!',\n",
       " 'goodness',\n",
       " 'bathroom',\n",
       " 'dennis',\n",
       " 'pm!',\n",
       " 'intrvw',\n",
       " 'awwww',\n",
       " 'optiscan',\n",
       " 'momento',\n",
       " 'premiere.',\n",
       " 'ago.',\n",
       " '*does*',\n",
       " 'cattle',\n",
       " 'photoes',\n",
       " '#sxsw-ers!',\n",
       " 'usa',\n",
       " 'lighters',\n",
       " 'gas',\n",
       " 'ck',\n",
       " 'rinna',\n",
       " 'if',\n",
       " 'someone',\n",
       " 'one!!',\n",
       " 'quotes',\n",
       " \"they're\",\n",
       " 'ipad..',\n",
       " 'market!',\n",
       " 'ipad2s',\n",
       " 'when',\n",
       " 'protip:',\n",
       " 'highlight',\n",
       " 'successful',\n",
       " '#sxswlatam',\n",
       " 'hitlantis',\n",
       " 'speakeasy',\n",
       " '#eurosxsw',\n",
       " '~5:30',\n",
       " '#androidsxsw',\n",
       " 'flight?',\n",
       " '#nfl',\n",
       " 'por',\n",
       " 'drowning.',\n",
       " 'margarita.',\n",
       " '(people',\n",
       " 'recommends',\n",
       " 'devices.',\n",
       " 'v2',\n",
       " 'front',\n",
       " 'blocked',\n",
       " 'media,',\n",
       " '2?\\n\\nnot',\n",
       " 'gram,',\n",
       " '#nerdsunite',\n",
       " 'promises',\n",
       " 'journal',\n",
       " 'days!',\n",
       " '#steamy',\n",
       " 'too',\n",
       " '#apple',\n",
       " 'too.',\n",
       " 'reporting:',\n",
       " 'disgusted',\n",
       " 'prepping',\n",
       " '#marketing',\n",
       " '(incl.',\n",
       " '#at&amp;t',\n",
       " 'choplifter',\n",
       " 'staying',\n",
       " 'location-aware',\n",
       " 'moment.',\n",
       " 'developer',\n",
       " 'paid',\n",
       " 'wanderer.',\n",
       " '(obv).',\n",
       " 'clipcon!',\n",
       " 'excuse',\n",
       " 'ipad/iphone',\n",
       " 'wanting',\n",
       " 'unconfirmed',\n",
       " 'dividends,',\n",
       " '#ctia',\n",
       " '#infektd',\n",
       " 'theatre',\n",
       " '#ausxsw',\n",
       " 'apple...',\n",
       " '2?',\n",
       " 'um,',\n",
       " 'estate',\n",
       " 'awesomeness',\n",
       " 'o',\n",
       " 'longer?',\n",
       " 'launch:',\n",
       " 'fingers.',\n",
       " 'comfort',\n",
       " 'enough',\n",
       " '#gps',\n",
       " 'africans,',\n",
       " 'stories:',\n",
       " 'experts',\n",
       " 'nyc',\n",
       " 'contextual',\n",
       " 'choice,',\n",
       " 'hmmm,',\n",
       " 'brah!',\n",
       " '(cont)',\n",
       " '#agchat',\n",
       " 'plate.',\n",
       " 'thunder.',\n",
       " '#kindle',\n",
       " '#mhealth',\n",
       " 'decide!',\n",
       " 'informed',\n",
       " 'speed',\n",
       " 'underwire',\n",
       " 'acceptable',\n",
       " 'castle',\n",
       " 'moonshine',\n",
       " 'usurped',\n",
       " '*everyone',\n",
       " 'photos',\n",
       " 'suppose...cheers',\n",
       " 'introduces',\n",
       " 'frm',\n",
       " 'religion',\n",
       " 'obs',\n",
       " 'innovators.',\n",
       " 'something.',\n",
       " 'backpack',\n",
       " 'sweet...',\n",
       " 'bit.ly/ea1zgd',\n",
       " 'there....{link}',\n",
       " 'smartest',\n",
       " 'scored',\n",
       " '#travel',\n",
       " 'year',\n",
       " 'mocked.',\n",
       " 'scenes',\n",
       " '-&gt;',\n",
       " '2,',\n",
       " 'lorry',\n",
       " '#thingsthatdontgotogether',\n",
       " 'huzzah!',\n",
       " '#sxsw@mention',\n",
       " 'meet-up.',\n",
       " 'given',\n",
       " '#marissameyer',\n",
       " '1)',\n",
       " 'exploiting',\n",
       " '@madebymany',\n",
       " 'performance:',\n",
       " '&quot;mobile',\n",
       " '#sxsw&quot;:',\n",
       " 'loving',\n",
       " '1.1',\n",
       " 'winner:',\n",
       " 'internetonlinewebsite.com,',\n",
       " '#droid',\n",
       " 'pagemaker',\n",
       " 'able',\n",
       " 'scarborough',\n",
       " 'anti',\n",
       " 'gamechanger',\n",
       " '#singularity',\n",
       " 'ooing',\n",
       " 'fresh',\n",
       " 'awesome!!',\n",
       " 'laptop',\n",
       " '#winwin',\n",
       " 'epic.',\n",
       " 'juice.',\n",
       " 'point!',\n",
       " 'usefulness',\n",
       " 'session.',\n",
       " 'very',\n",
       " 'gone',\n",
       " 'activate',\n",
       " 'wrong!',\n",
       " 'launching',\n",
       " 'being',\n",
       " 'single',\n",
       " '&quot;route-around&quot;',\n",
       " 'group/graphic.ly',\n",
       " 'jeebus',\n",
       " 'express',\n",
       " 'positives-',\n",
       " '23',\n",
       " 'playstation',\n",
       " 'line?',\n",
       " 'rigeur.',\n",
       " 'auth',\n",
       " 'conv',\n",
       " '(although',\n",
       " 'clark',\n",
       " 'blast!',\n",
       " 'saying.',\n",
       " 'synching',\n",
       " 'intelligence',\n",
       " 'ran',\n",
       " '#longlinesbadux',\n",
       " 'yep.',\n",
       " 'clean,',\n",
       " 'discusses',\n",
       " 'loathe',\n",
       " 'friday,',\n",
       " 'dang',\n",
       " 'surprise',\n",
       " 'on.',\n",
       " 'focus?',\n",
       " 'world',\n",
       " \"clark's\",\n",
       " 'sync.',\n",
       " 'you!!!!',\n",
       " 'pop-up!',\n",
       " 'bird',\n",
       " 'affair.\\n#sxsw',\n",
       " 'explorers.',\n",
       " 'upgrade.',\n",
       " 'avoid',\n",
       " 'admired',\n",
       " 'proof',\n",
       " 'rock!!!',\n",
       " 'years!',\n",
       " 'blackberry???',\n",
       " '5,000',\n",
       " 'preferred',\n",
       " '[softlayer',\n",
       " 'action&quot;',\n",
       " '(hopefully',\n",
       " 'hipsters',\n",
       " 'month',\n",
       " '(no',\n",
       " 'copia',\n",
       " 'purchases.',\n",
       " 'one&quot;',\n",
       " 'events.',\n",
       " 'flood',\n",
       " 'code',\n",
       " 'pc',\n",
       " 'attend',\n",
       " '&lt;---cool',\n",
       " 'conference.',\n",
       " 'reality.',\n",
       " 'giggle',\n",
       " 'keep',\n",
       " 'route-around',\n",
       " 'madness.',\n",
       " 'here...',\n",
       " 'hooting',\n",
       " 'attendees)',\n",
       " 'queue',\n",
       " '#webvisions',\n",
       " 'envy.',\n",
       " 'tattooed',\n",
       " 'swift',\n",
       " '&quot;apple',\n",
       " 'lines,',\n",
       " 'picking',\n",
       " 'packs',\n",
       " 'schools.',\n",
       " 'paul',\n",
       " 'with.',\n",
       " 'downloads',\n",
       " '1?',\n",
       " 'ads',\n",
       " 'that...',\n",
       " 'korean',\n",
       " '#random',\n",
       " 'unless',\n",
       " \"you'll\",\n",
       " '#lonelyplanet',\n",
       " 'applause',\n",
       " 'mexican',\n",
       " '#aus',\n",
       " \"mayer's\",\n",
       " '&quot;before',\n",
       " 'rpg',\n",
       " 'noticed',\n",
       " 'beautiful',\n",
       " 'knockout',\n",
       " 'kenny,',\n",
       " 'laser',\n",
       " '#project314',\n",
       " 'who',\n",
       " 'wary',\n",
       " 'christmas.',\n",
       " 'introduced',\n",
       " 'fans',\n",
       " '1/4',\n",
       " \"'fast,\",\n",
       " 'done',\n",
       " 'frothy',\n",
       " 'grumbling',\n",
       " 'austin-bound',\n",
       " 'named',\n",
       " 'agents',\n",
       " 'ugh.',\n",
       " 'us:',\n",
       " 'an',\n",
       " 'spoke',\n",
       " 'line.',\n",
       " 'now',\n",
       " 'kid.',\n",
       " 'important',\n",
       " '#adpeopleproblems',\n",
       " 'corporation',\n",
       " '#playhopskoch',\n",
       " '3)',\n",
       " 'academy',\n",
       " 'values',\n",
       " 'fam...',\n",
       " \"'get\",\n",
       " 'mad',\n",
       " '#dairy',\n",
       " 'hidden',\n",
       " 'once',\n",
       " 'facebook.com/powermat',\n",
       " 'sucks.',\n",
       " 'gives',\n",
       " '\\nreal',\n",
       " 'sweater',\n",
       " 'beer,',\n",
       " 'sandwiched',\n",
       " 'general',\n",
       " 'rebecca',\n",
       " 'behave',\n",
       " '&quot;...by',\n",
       " '4-year',\n",
       " \"'where'\",\n",
       " 'v.',\n",
       " 'comes',\n",
       " 'urs.',\n",
       " 'folks.',\n",
       " 'all!',\n",
       " 'asleep',\n",
       " 'generated',\n",
       " 'fusion',\n",
       " 'solution',\n",
       " 'direct',\n",
       " 'pure',\n",
       " 'virgin,',\n",
       " 'watch:',\n",
       " 'trends:\\nout:',\n",
       " 'studios',\n",
       " '96',\n",
       " 'delayed',\n",
       " 'party...awesome.',\n",
       " 'groups,',\n",
       " 'now!',\n",
       " 'enough&quot;',\n",
       " '4.3',\n",
       " 'actsofsharing.com',\n",
       " 'surplus',\n",
       " 'zero.',\n",
       " '#frostwire',\n",
       " 'ubiquity',\n",
       " 'barroom',\n",
       " 'hm?',\n",
       " 'simple',\n",
       " 'brilliant...i',\n",
       " 'win-win!',\n",
       " 'bajillions',\n",
       " '40min',\n",
       " 'good.i',\n",
       " 'thing',\n",
       " 'kindle',\n",
       " 'early.',\n",
       " 'hive-think.&quot;',\n",
       " '59p',\n",
       " 'hotel',\n",
       " 'anyone?',\n",
       " 'unadulterated',\n",
       " 'et',\n",
       " '#cm48',\n",
       " '(thanks',\n",
       " \"how's\",\n",
       " 'users.',\n",
       " 'to?',\n",
       " 'chumps?',\n",
       " 'city',\n",
       " 'larger',\n",
       " 'tunehopper.',\n",
       " 'cords...',\n",
       " 'harlow:',\n",
       " '&quot;making',\n",
       " 'sale!',\n",
       " 'smudgy',\n",
       " 'forgotten.',\n",
       " 'sweets?',\n",
       " 'brilliant',\n",
       " 'ios',\n",
       " 'line,',\n",
       " 'hunt:',\n",
       " '#nerd',\n",
       " 'converge',\n",
       " '#nptech',\n",
       " 'stand',\n",
       " \"couldn't\",\n",
       " 'tech-savvy',\n",
       " 'drop,',\n",
       " 'tech.',\n",
       " 'rsvp',\n",
       " 'actions',\n",
       " '&quot;transparency,',\n",
       " 'pilhofer',\n",
       " '#alwayshavingtoplugin',\n",
       " 'normal',\n",
       " 'cry',\n",
       " 'midday,',\n",
       " 'join',\n",
       " 'personalized',\n",
       " 'sell.',\n",
       " '#mitharvard',\n",
       " 'camera!',\n",
       " 'what???',\n",
       " 'run',\n",
       " 'blame',\n",
       " 'insane.',\n",
       " 'full-force!',\n",
       " 'she',\n",
       " 'photos,',\n",
       " 'antonio.',\n",
       " 'fan&quot;',\n",
       " '&lt;thanks!installed',\n",
       " 'attitudes',\n",
       " '#notionink',\n",
       " 'us!',\n",
       " 'image',\n",
       " 'endeavor.',\n",
       " 'eminent.',\n",
       " 'b.s.',\n",
       " '2011&quot;',\n",
       " \"year'\",\n",
       " \"hasn't\",\n",
       " 'recipe',\n",
       " 'removable',\n",
       " 'squeeze',\n",
       " 'idea...',\n",
       " 'coming',\n",
       " '&gt;&gt;really',\n",
       " \"'physical\",\n",
       " 'cant',\n",
       " 'easier',\n",
       " 'pack.',\n",
       " 'seems.',\n",
       " 'done.',\n",
       " 'us,',\n",
       " 'utter...',\n",
       " 'oil.',\n",
       " 'focuses',\n",
       " 'hard.',\n",
       " 'syked',\n",
       " 'fellow',\n",
       " '8',\n",
       " 'radical',\n",
       " 'idea',\n",
       " 'screenings',\n",
       " 'telegraph',\n",
       " \"doesn't\",\n",
       " 'awesomeness!',\n",
       " 'so',\n",
       " 'coincide',\n",
       " 'layer',\n",
       " 'wait)',\n",
       " 'incl',\n",
       " 'adopter?',\n",
       " 'cash',\n",
       " 'might',\n",
       " 'dow',\n",
       " 'spy',\n",
       " 'radio',\n",
       " 'tablets.',\n",
       " 'rock.)',\n",
       " 'psfk',\n",
       " 'crappy',\n",
       " '-mayer.',\n",
       " 'network.',\n",
       " '!',\n",
       " 'releasing',\n",
       " '3k',\n",
       " 'dealing',\n",
       " 'geeking',\n",
       " 'cloud',\n",
       " 'afternoon',\n",
       " 'that?',\n",
       " '#11ntc',\n",
       " 'services.',\n",
       " 'that!',\n",
       " 'brought',\n",
       " '&quot;thank',\n",
       " 'bear-creatures',\n",
       " '#looseorganizations',\n",
       " 'litle',\n",
       " 'hey',\n",
       " 'up',\n",
       " 'itself',\n",
       " \"who's\",\n",
       " 'early',\n",
       " 'essentials',\n",
       " '#ipad,',\n",
       " 'mmm.',\n",
       " 'dst',\n",
       " 'craps',\n",
       " 'showed',\n",
       " 'planzai',\n",
       " '#augmentedreality',\n",
       " '(kingston)',\n",
       " 'inbox',\n",
       " 'protecting',\n",
       " 'grant',\n",
       " 'on,',\n",
       " 'lot',\n",
       " 'fix',\n",
       " 'insatiable,',\n",
       " 'enchanted.',\n",
       " 'iphone!!',\n",
       " 'global',\n",
       " '(taken',\n",
       " 'hordes',\n",
       " 'techie',\n",
       " '#bettersearch.',\n",
       " '(ie:',\n",
       " 'close',\n",
       " 'msft',\n",
       " 'obsolete?',\n",
       " 'office!',\n",
       " 'fades,',\n",
       " 'side',\n",
       " 'you!',\n",
       " 'pdf&quot;',\n",
       " 'remaining',\n",
       " 'pariah',\n",
       " '#rji',\n",
       " 'station',\n",
       " 'pop-up',\n",
       " 'louisiana.',\n",
       " 'handy',\n",
       " 'please!',\n",
       " 're:new',\n",
       " 'hrs',\n",
       " 'delay',\n",
       " 'prizes.',\n",
       " 'liveblog',\n",
       " 'code!)',\n",
       " '(vs.',\n",
       " 'shop!',\n",
       " 'cnet',\n",
       " 'jeans,',\n",
       " 'dogs?)',\n",
       " 'hearing',\n",
       " 'nab',\n",
       " 'door',\n",
       " 'breathtaking',\n",
       " 'yield',\n",
       " 'etsy',\n",
       " '#tbwasxsw',\n",
       " 'television,',\n",
       " 'my',\n",
       " 'yonkers.',\n",
       " 'store:',\n",
       " 'enable',\n",
       " 'spin.com',\n",
       " 'methinks',\n",
       " 'complex',\n",
       " 'over...',\n",
       " 'losers',\n",
       " 'uppward)',\n",
       " 'finally...',\n",
       " 'mophie',\n",
       " 'first.',\n",
       " 'revolutions.',\n",
       " 'cor.',\n",
       " \"brazil's\",\n",
       " 'ipad/tablet',\n",
       " '#filmaster',\n",
       " '#smartphone',\n",
       " 'woah.',\n",
       " 'approved',\n",
       " 'handset.',\n",
       " 'shows',\n",
       " 'cannot',\n",
       " '#midem.',\n",
       " '#sxsw-',\n",
       " 'parked',\n",
       " 'experiment.',\n",
       " 'brisk',\n",
       " 'appears',\n",
       " 'laptop?',\n",
       " 'alert',\n",
       " '36',\n",
       " '.',\n",
       " 'cc',\n",
       " 'hand.',\n",
       " 'remember',\n",
       " '#hotpot',\n",
       " 'tools',\n",
       " 'hands,',\n",
       " 'searches',\n",
       " '&quot;my',\n",
       " 'suckling',\n",
       " 'blurs,',\n",
       " '#s',\n",
       " 'which',\n",
       " 'song',\n",
       " 'rite',\n",
       " 'them,',\n",
       " 'others',\n",
       " 'ass',\n",
       " '\\n\\napple',\n",
       " 'ipad-winning',\n",
       " 'rumours',\n",
       " 'tomorrow,',\n",
       " 'ts',\n",
       " 'powerful',\n",
       " 'overheard',\n",
       " 'seated',\n",
       " 'out,',\n",
       " 'left...crossing',\n",
       " '#pickmeupanipad2',\n",
       " 'two?',\n",
       " 'tweeps!',\n",
       " 'unbelievable',\n",
       " 'balckberries',\n",
       " 'rest.',\n",
       " 'addition',\n",
       " 'crasher&quot;',\n",
       " 'francisco',\n",
       " 'ride',\n",
       " 'campaigns,',\n",
       " 'panel,',\n",
       " 'improvement',\n",
       " 'down.',\n",
       " 'artwork.',\n",
       " 'real?',\n",
       " '@hamsandwich',\n",
       " 'enjoy',\n",
       " \"wouldn't\",\n",
       " 'ridiculous:',\n",
       " 'thru',\n",
       " 'domain',\n",
       " 'hotpot',\n",
       " 'nice',\n",
       " 'sux.',\n",
       " 'the.',\n",
       " '#tablet',\n",
       " 'advent',\n",
       " 'rockin',\n",
       " 'bernd,',\n",
       " 'why',\n",
       " 'thousands',\n",
       " 'freeze',\n",
       " 'such',\n",
       " 'off',\n",
       " 'filming',\n",
       " 'bestie',\n",
       " 'essentially',\n",
       " '#sxswpass',\n",
       " 'friday.\\nipad',\n",
       " '#hereforwork',\n",
       " 'hours',\n",
       " 'auto-correct?',\n",
       " 'cc:',\n",
       " 'shrink',\n",
       " 'awesome,',\n",
       " 'generations',\n",
       " 'pie',\n",
       " 'natural,',\n",
       " 'v2.',\n",
       " 'sheen.',\n",
       " 'lucky',\n",
       " 'mayer.',\n",
       " 'ferriss,',\n",
       " '#tonchidot',\n",
       " '$$',\n",
       " 'disney',\n",
       " 'deal!',\n",
       " 'leather',\n",
       " 'reality',\n",
       " 'swisher',\n",
       " 'hotpot.',\n",
       " '#saysshewithoutanipad',\n",
       " 'neumann',\n",
       " 'chargers.',\n",
       " '#psych',\n",
       " 'swag,',\n",
       " 'fail',\n",
       " 'listen.',\n",
       " 'charger!',\n",
       " 'vid',\n",
       " 'ignite',\n",
       " '4).',\n",
       " 'much!!!!',\n",
       " 'over',\n",
       " 'changing',\n",
       " 'really',\n",
       " 'matching',\n",
       " \"park's\",\n",
       " 'out.&quot;',\n",
       " 'voice.',\n",
       " '@jessedee',\n",
       " '#behance',\n",
       " 'salon',\n",
       " 'words',\n",
       " 'petricone',\n",
       " '#maps',\n",
       " \"demo's\",\n",
       " 'wish:',\n",
       " '#checkins',\n",
       " '&lt;-',\n",
       " 'detection',\n",
       " 'tablet.',\n",
       " \"wasn't\",\n",
       " 'analytics',\n",
       " 'mind&quot;',\n",
       " 'areas',\n",
       " 'a',\n",
       " 'sorted.',\n",
       " 'welcome.',\n",
       " '--marissa',\n",
       " '3/20',\n",
       " 'crashy',\n",
       " 'invest',\n",
       " 'ridculous!',\n",
       " \"we're\",\n",
       " '%^*%#!!',\n",
       " 'doodle',\n",
       " 'skip',\n",
       " \"&quot;it's\",\n",
       " 'saying',\n",
       " 'evil.&quot;',\n",
       " 'ze',\n",
       " 'invites!',\n",
       " 'japan',\n",
       " 'water/energy',\n",
       " 'organize',\n",
       " 'crisis',\n",
       " '#startups',\n",
       " 'uberguide',\n",
       " 'rainjacket,',\n",
       " 'overall,',\n",
       " 'http://bit.ly/etsbzk',\n",
       " 'crowd!',\n",
       " '#gamelayer',\n",
       " '#gitchococktailon',\n",
       " 'putting',\n",
       " '#sxfl',\n",
       " 'awards.',\n",
       " 'born!',\n",
       " '&quot;it',\n",
       " 'blackberry!',\n",
       " 'tear.',\n",
       " \"'so\",\n",
       " 'etc',\n",
       " 'robot',\n",
       " 'speak,',\n",
       " '35-hour',\n",
       " 'beauty',\n",
       " 'lat',\n",
       " 'slick',\n",
       " 'genius.',\n",
       " 'services.&quot;',\n",
       " '#googlemaps',\n",
       " 'browsing.',\n",
       " 'opens',\n",
       " '150',\n",
       " '&quot;arg!',\n",
       " 'member',\n",
       " 'days,',\n",
       " 'clearly.',\n",
       " 'hints',\n",
       " 'drive',\n",
       " 'condense',\n",
       " 'relief',\n",
       " 'farooqui:',\n",
       " 'skynet.',\n",
       " 'priorities',\n",
       " 'seat',\n",
       " 'hotpot!',\n",
       " '$,',\n",
       " 'improvements...',\n",
       " 'helped',\n",
       " 'planting',\n",
       " 'sadly',\n",
       " 'offline',\n",
       " 'likes!',\n",
       " 'enables',\n",
       " 'html5,',\n",
       " 'lovin',\n",
       " 'lightbulb',\n",
       " 'pun',\n",
       " 'vending',\n",
       " 'packed.',\n",
       " 'plane',\n",
       " '#',\n",
       " 'capped',\n",
       " 'kid',\n",
       " 'mystery',\n",
       " 'envisioning',\n",
       " 'paper',\n",
       " 'tethering',\n",
       " 'can,',\n",
       " 'initiative',\n",
       " 'recreated',\n",
       " '#pseudoretweet',\n",
       " 'morning...',\n",
       " 'lab',\n",
       " 'goin',\n",
       " 'cable',\n",
       " 'alarm.',\n",
       " 'wam/night',\n",
       " 'bug',\n",
       " 'microsoft,',\n",
       " 'ipad?!',\n",
       " 'dan',\n",
       " '16mins:',\n",
       " 'utilize',\n",
       " 'sweeeet!',\n",
       " 'sxsw,',\n",
       " 'moment&quot;',\n",
       " 'kiddie',\n",
       " 'want!',\n",
       " 'action:',\n",
       " ...]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniqueWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ShA6D8jKH0N5"
   },
   "source": [
    "### 6. Find out how many Positive and Negative emotions are there.\n",
    "\n",
    "Hint: Use value_counts on that column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q7LAl5pzH0N6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positive emotion    2672\n",
       "Negative emotion     519\n",
       "Name: is_there_an_emotion_directed_at_a_brand_or_product, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['is_there_an_emotion_directed_at_a_brand_or_product'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IUvgj0FoH0N9"
   },
   "source": [
    "### 7. Change the labels for Positive and Negative emotions as 1 and 0 respectively and store in a different column in the same dataframe named 'Label'\n",
    "\n",
    "Hint: use map on that column and give labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YftKwFv7H0N9"
   },
   "outputs": [],
   "source": [
    "data['Label'] = data['is_there_an_emotion_directed_at_a_brand_or_product']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_values = {'Positive emotion': 1, 'Negative emotion': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Label = data.Label.map(new_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "      <th>text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iPhone   \n",
       "1              iPad or iPhone App   \n",
       "2                            iPad   \n",
       "3              iPad or iPhone App   \n",
       "4                          Google   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \\\n",
       "0                                   Negative emotion   \n",
       "1                                   Positive emotion   \n",
       "2                                   Positive emotion   \n",
       "3                                   Negative emotion   \n",
       "4                                   Positive emotion   \n",
       "\n",
       "                                                text  Label  \n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...      0  \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...      1  \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...      1  \n",
       "3  @sxsw I hope this year's festival isn't as cra...      0  \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...      1  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3YErwYLCH0N_"
   },
   "source": [
    "### 8. Define the feature set (independent variable or X) to be `text` column and `labels` as target (or dependent variable)  and divide into train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = data.text\n",
    "y = data.Label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lNkwrGgEH0OA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2233,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(958,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q5nlCuaaH0OD"
   },
   "source": [
    "## 9. **Predicting the sentiment:**\n",
    "\n",
    "\n",
    "### Use Naive Bayes and Logistic Regression and print their accuracy scores for predicting the sentiment of the given text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2AbVYssaH0OE"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ktXrLhmOH0Of"
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer()\n",
    "\n",
    "# create document-term matrices\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "clv2X0kKH0Ok"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2233, 4678)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K86LRMfdH0Ou"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(958, 4678)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  4678\n"
     ]
    }
   ],
   "source": [
    "print('Features: ', X_train_dtm.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_dtm, y_train)\n",
    "y_pred_class = nb.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8695198329853863\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "print (metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8830897703549061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Paridhi\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# use logistic regression with text column only\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train_dtm, y_train)\n",
    "y_pred_class = logreg.predict(X_test_dtm)\n",
    "print (metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sw-0B33tH0Ox"
   },
   "source": [
    "## 10. Create a function called `tokenize_test` which can take count vectorizer object as input, create document term matrix out of x_train & x_test, build and train a model using dtm created and print the accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "okCTOs1TH0Oy"
   },
   "outputs": [],
   "source": [
    "def tokenize_test(vect):\n",
    "    X_train_dtm = vect.fit_transform(X_train)\n",
    "    print('Features: ', X_train_dtm.shape[1])\n",
    "    X_test_dtm = vect.transform(X_test)\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train_dtm, y_train)\n",
    "    y_pred_class = nb.predict(X_test_dtm)\n",
    "    print('Accuracy: ', metrics.accuracy_score(y_test, y_pred_class))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JxZ8jfPEH0O0"
   },
   "source": [
    "### Create a count vectorizer function which includes n_grams = 1,2  and pass it to tokenize_test function to print the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kdCyAN_IH0O0"
   },
   "outputs": [],
   "source": [
    "vect1 = CountVectorizer(ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  23443\n",
      "Accuracy:  0.8736951983298539\n"
     ]
    }
   ],
   "source": [
    "tokenize_test(vect1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "axepytmgH0O4"
   },
   "source": [
    "### Create a count vectorizer function with stopwords = 'english'  and pass it to tokenize_test function to print the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HToGkq7vH0O4"
   },
   "outputs": [],
   "source": [
    "vect2 = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  4438\n",
      "Accuracy:  0.8716075156576201\n"
     ]
    }
   ],
   "source": [
    "tokenize_test(vect2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iOIlJRxoH0O7"
   },
   "source": [
    "### Create a count vectorizer function with stopwords = 'english' and max_features =300  and pass it to tokenize_test function to print the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6fUhff-oH0O8"
   },
   "outputs": [],
   "source": [
    "vect3 = CountVectorizer(stop_words='english',max_features= 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  300\n",
      "Accuracy:  0.824634655532359\n"
     ]
    }
   ],
   "source": [
    "tokenize_test(vect3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S2KZNWVkH0PA"
   },
   "source": [
    "### Create a count vectorizer function with n_grams = 1,2  and max_features = 15000  and pass it to tokenize_test function to print the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3v9XD082H0PB"
   },
   "outputs": [],
   "source": [
    "vect4 = CountVectorizer(ngram_range=(1, 2),max_features= 15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  15000\n",
      "Accuracy:  0.8716075156576201\n"
     ]
    }
   ],
   "source": [
    "tokenize_test(vect4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "We3JK_SRH0PO"
   },
   "source": [
    "### Create a count vectorizer function with n_grams = 1,2  and include terms that appear at least 2 times (min_df = 2)  and pass it to tokenize_test function to print the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fUHrfDCyH0PP"
   },
   "outputs": [],
   "source": [
    "vect5 = CountVectorizer(ngram_range=(1, 2),min_df=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3H4k_lVZH0PS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  7101\n",
      "Accuracy:  0.8674321503131524\n"
     ]
    }
   ],
   "source": [
    "tokenize_test(vect5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5VltP3aeMvrW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "sentiment_analysis_twitter_data_questions.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
